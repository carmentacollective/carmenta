# Carmenta Evaluation Baseline - December 24, 2024

## Metadata

- **Timestamp**: 2024-12-24 (run at ~10:47 PST)
- **Git Commit**: dc5e2e8e1dc7055b3e918e109bfb6ae4cc5484cd
- **Environment**: Development server running on localhost:3000
- **Braintrust API Key**: Configured
- **Test User Token**: Configured

## Summary

| Eval Suite  | Total Cases    | Passed | Failed | Success Rate | Duration  | Status        |
| ----------- | -------------- | ------ | ------ | ------------ | --------- | ------------- |
| Routing     | 25             | 0      | 25     | 0%           | 300.98s   | ❌ All Failed |
| Concierge   | 32 (x4 models) | 32     | 0      | 100%         | 2.95s avg | ✅ Pass       |
| Attachments | 6              | 0      | 6      | 0%           | 300.95s   | ❌ All Failed |
| Competitive | 25             | 0      | 25     | 0%           | 301s      | ❌ All Failed |

## Detailed Results

### 1. Routing Eval ❌

**Experiment**:
[main-1766590048](https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Routing/experiments/main-1766590048)

**Status**: 25/25 cases failed with errors

**Key Metrics vs Baseline** (main-1766507868):

- Errors: 2 → 27 (+200.00%, 25 regressions)
- Duration: 300.98s (+26773.40%)
- All LLM metrics: 0 (no successful completions)

**Analysis**: All test cases timed out or errored. The 300s duration suggests timeout
issues. No LLM calls were made, indicating the requests never reached the model layer.

**Braintrust URL**:
https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Routing/experiments/main-1766590048

---

### 2. Concierge Eval ✅

**Status**: All models passing with excellent metrics

#### Claude Haiku 4.5

**Experiment**:
[main-1766590683](https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Concierge%20-%20Claude%20Haiku%204.5/experiments/main-1766590683)

**Key Metrics vs Baseline** (main-1766504066):

- Latency: 22.50% (0.00% change)
- Model Selection: 90.00% (-5.00%, 1 regression)
- Title Generated: 100.00% (0.00% change)
- Valid Output: 100.00% (0.00% change)
- Reasoning: 100.00% (+7.69%, 1 improvement)
- Temperature: 100.00% (0.00% change)
- Title Length: 100.00% (0.00% change)
- Title Pattern: 100.00% (0.00% change)
- Auto Switch: 100.00% (0.00% change)
- Duration: 2.95s (-5.15%)

#### GPT-5 Mini

**Experiment**:
[main-1766590683](https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Concierge%20-%20GPT-5%20Mini/experiments/main-1766590683)

**Key Metrics vs Baseline** (main-1766504065):

- Latency: 22.50% (0.00% change)
- Model Selection: 92.50% (+2.50%, 1 improvement)
- Title Generated: 100.00% (0.00% change)
- Valid Output: 100.00% (0.00% change)
- Reasoning: 100.00% (+7.69%, 1 improvement)
- Temperature: 100.00% (0.00% change)
- Title Length: 100.00% (0.00% change)
- Title Pattern: 100.00% (0.00% change)
- Auto Switch: 100.00% (0.00% change)
- Duration: 3.32s (-3.39%)

#### Grok 4.1 Fast

**Experiment**:
[main-1766590683](https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Concierge%20-%20Grok%204.1%20Fast/experiments/main-1766590683)

**Key Metrics vs Baseline** (main-1766504067):

- Latency: 22.50% (0.00% change)
- Model Selection: 87.50% (-7.50%, 3 regressions)
- Title Generated: 100.00% (0.00% change)
- Valid Output: 100.00% (0.00% change)
- Reasoning: 100.00% (+7.69%, 1 improvement)
- Temperature: 100.00% (0.00% change)
- Title Length: 100.00% (0.00% change)
- Title Pattern: 100.00% (0.00% change)
- Auto Switch: 100.00% (0.00% change)
- Duration: 3.11s (+1.87%)

#### Gemini 3 Pro

**Experiment**:
[main-1766590683](https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Concierge%20-%20Gemini%203%20Pro/experiments/main-1766590683)

**Key Metrics vs Baseline** (main-1766504068):

- Latency: 22.50% (0.00% change)
- Model Selection: 90.00% (-5.00%, 2 regressions)
- Title Generated: 100.00% (0.00% change)
- Valid Output: 100.00% (0.00% change)
- Reasoning: 100.00% (+7.69%, 1 improvement)
- Temperature: 100.00% (0.00% change)
- Title Length: 100.00% (0.00% change)
- Title Pattern: 100.00% (0.00% change)
- Auto Switch: 100.00% (0.00% change)
- Duration: 3.04s (+4.37%)

**Analysis**: The concierge eval is performing excellently across all models. Minor
variations in model selection (-5% to +2.5%) but core functionality (title generation,
valid output, reasoning) at 100%. The reasoning metric improved by 7.69% across all
models.

---

### 3. Attachments Eval ❌

**Experiment**:
[main-1766590725](https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Attachments/experiments/main-1766590725)

**Status**: 6/6 cases failed

**Key Metrics vs Baseline** (main-1765730741):

- HTTP Success: 0.00% (-100.00%, 6 regressions)
- Model Routing: 0.00% (-100.00%, 6 regressions)
- Response Content: 0.00% (-100.00%, 2 regressions)
- Duration: 300.95s (+29133.32%)

**Analysis**: Complete failure with 300s timeouts. All HTTP requests failing, suggesting
the attachment upload/processing endpoints are timing out or unavailable.

**Braintrust URL**:
https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Attachments/experiments/main-1766590725

---

### 4. Competitive Eval ❌

**Experiment**:
[main-1766591074](https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Competitive%20Benchmark/experiments/main-1766591074)

**Status**: 25/25 cases failed with errors

**Key Metrics vs Baseline** (evals-run-20251212-1765582597):

- Errors: 2 → 27 (+200.00%, 25 regressions)
- Duration: 301s (+24713.89%)
- All LLM metrics: 0 (no successful completions)

**Analysis**: Similar to routing eval - all cases timing out at ~300s with no successful
LLM calls.

**Braintrust URL**:
https://www.braintrust.dev/app/Carmenta%20Collective/p/Carmenta%20Competitive%20Benchmark/experiments/main-1766591074

---

## Common Patterns

### Successes ✅

- **Concierge eval**: Stable, fast, 100% success rate across all models
- Core routing logic for model selection works well
- Title generation and validation robust

### Failures ❌

- **Timeout Pattern**: Routing, Attachments, and Competitive evals all timing out at
  ~300s
- **No LLM Calls**: Failed tests show 0 LLM calls, suggesting requests never reach the
  model
- **HTTP Failures**: Attachments eval shows complete HTTP failure (0% success)

## Hypotheses for Failures

1. **Authentication Issues**: Tests may be failing auth checks, causing early rejection
2. **API Endpoint Timeouts**: The chat/streaming endpoints may have increased latency or
   be hanging
3. **Test Configuration**: Test user token or API keys may need refresh
4. **Network/Infrastructure**: Possible rate limiting or connectivity issues to
   OpenRouter

## Next Steps

1. **Investigate timeout root cause**: Check server logs for requests that timeout
2. **Verify auth flow**: Ensure TEST_USER_TOKEN is valid and auth middleware working
3. **Check API endpoints**: Test chat and attachment endpoints manually
4. **Review test configuration**: Verify test setup matches current API contracts
5. **Add more granular logging**: Instrument the eval code to see where failures occur

## Notes

- All evals used AI SDK v2 specification compatibility mode (expected warnings)
- Dev server was running throughout all tests
- Environment variables (BRAINTRUST_API_KEY, TEST_USER_TOKEN) properly configured
- Baseline comparisons show significant regressions in 3/4 suites since last successful
  run
